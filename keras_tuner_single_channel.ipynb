{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Read in packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 16:08:09.965525: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-18 16:08:11.753507: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.local/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_669006/366055844.py:18: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras_unet_collection import models\n",
    "import tensorflow_addons as tfa\n",
    "import optuna\n",
    "from optkeras.optkeras import OptKeras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "import kerastuner as kt\n",
    "# from tensorflow import tensorflow.keras.mixed_precision.set_global_policy(\"mixed_float16\")x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396841"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#get all the pathways\n",
    "training_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj_0_128_training_files.csv')['Files'].tolist()\n",
    "validation_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj_0_128_validation_files.csv')['Files'].tolist()\n",
    "testing_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj_0_128_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "# #get negative name3s\n",
    "# training_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtn_negative_images_crop_subs_128_training_files.csv')['Files'].tolist()\n",
    "# validation_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtn_negative_images_crop_subs_128_validation_files.csv')['Files'].tolist()\n",
    "# testing_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtn_negative_images_crop_subs_128_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "# training_names3 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/random_negative_images_subs_128_training_files.csv')['Files'].tolist()\n",
    "# validation_names3 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/random_negative_images_subs_128_validation_files.csv')['Files'].tolist()\n",
    "# testing_names3 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/random_negative_images_subs_128_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "# training_names = training_names + training_names2 + training_names3\n",
    "# validation_names = validation_names + validation_names2 + validation_names3\n",
    "# testing_names = testing_names + testing_names2 + testing_names3\n",
    "\n",
    "min_max = pd.read_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_global_min_max_cutoff_proj.csv\").reset_index(drop = True)\n",
    "\n",
    "min_max = min_max[['6']]\n",
    "\n",
    "len(training_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up image generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    training_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj_0_128_training_files.csv')['Files']\n",
    "    validation_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj_0_128_validation_files.csv')['Files']\n",
    "    testing_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj_0_128_testing_files.csv')['Files']\n",
    "\n",
    "#     training_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtn_negative_images_crop_subs_128_training_files.csv')['Files'].tolist()\n",
    "#     validation_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtn_negative_images_crop_subs_128_validation_files.csv')['Files'].tolist()\n",
    "#     testing_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtn_negative_images_crop_subs_128_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "#     training_names3 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/random_negative_images_subs_128_training_files.csv')['Files'].tolist()\n",
    "#     validation_names3 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/random_negative_images_subs_128_validation_files.csv')['Files'].tolist()\n",
    "#     testing_names3 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/random_negative_images_subs_128_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "#     training_names = training_names + training_names2 + training_names3\n",
    "#     validation_names = validation_names + validation_names2 + validation_names3\n",
    "#     testing_names = testing_names + testing_names2 + testing_names3\n",
    "\n",
    "    min_max = pd.read_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_global_min_max_cutoff_proj.csv\").reset_index(drop = True)\n",
    "\n",
    "    min_max = min_max[['6']]\n",
    "\n",
    "    #read in the trainiing and testing files and get lissts\n",
    "    train_files = training_names[:100]\n",
    "    val_files = validation_names[:100]\n",
    "    test_files = testing_names[:100]\n",
    "\n",
    "    #'Generates data containing batch_size samples' \n",
    "    \n",
    "    #-------first get training data\n",
    "    x_train = np.empty((len(train_files),128,128),  dtype=\"float32\")\n",
    "    y_train = np.empty((len(train_files),128,128,1),  dtype=\"uint8\")\n",
    "    \n",
    "    # x_train = np.empty((101,128,128,9),  dtype=\"float32\")\n",
    "    # y_train = np.empty((101,128,128,1),  dtype=\"uint8\")\n",
    "    \n",
    "    #start populating x by enumerating over the input img paths\n",
    "    for j, path in enumerate(train_files):\n",
    "\n",
    "        #load image\n",
    "        img =  np.round(np.load(path), 3)[:, :, 6]\n",
    "\n",
    "        # img = img * 1000\n",
    "        img = img.astype(float)\n",
    "        img = np.round(img, 3)\n",
    "        img[img == 0] = -999\n",
    "\n",
    "        img[np.isnan(img)] = -999\n",
    "\n",
    "\n",
    "        img[img == -999] = np.nan\n",
    "\n",
    "        in_shape = img.shape\n",
    "\n",
    "        #turn to dataframe to normalize\n",
    "        img = img.reshape(img.shape[0] * img.shape[1])\n",
    "\n",
    "        img = pd.DataFrame(img)\n",
    "\n",
    "        img.columns = min_max.columns\n",
    "\n",
    "        img = pd.concat([min_max, img]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "        #normalize 0 to 1\n",
    "        img = pd.DataFrame(scaler.fit_transform(img))\n",
    "\n",
    "        img = img.iloc[2:]\n",
    "\n",
    "        img = img.values.reshape(in_shape)\n",
    "\n",
    "        img[np.isnan(img)] = -1\n",
    "\n",
    "\n",
    "\n",
    "        img = np.round(img, 3)\n",
    "\n",
    "        x_train[j] = img#[:, :, 4:] index number is not included, \n",
    "\n",
    "    for j, path in enumerate(train_files):\n",
    "\n",
    "        #load image\n",
    "        img =  np.round(np.load(path), 3)[:, :, -1]\n",
    "\n",
    "        img = img.astype(int)\n",
    "\n",
    "        img[img < 0] = 0\n",
    "        img[img >1] = 0\n",
    "        img[~np.isin(img, [0,1])] = 0\n",
    "\n",
    "        img[np.isnan(img)] = 0\n",
    "        img = img.astype(int)\n",
    "\n",
    "        y_train[j] = np.expand_dims(img, 2) \n",
    "\n",
    "        \n",
    "    x_val = np.empty((len(val_files),128,128),  dtype=\"float32\")\n",
    "    y_val = np.empty((len(val_files),128,128,1),  dtype=\"uint8\")\n",
    "\n",
    "    \n",
    "    for j, path in enumerate(val_files):\n",
    "\n",
    "        img =  np.round(np.load(path), 3)[:, :, 6]\n",
    "\n",
    "        img = img.astype(float)\n",
    "        img = np.round(img, 3)\n",
    "        img[img == 0] = -999\n",
    "\n",
    "        img[np.isnan(img)] = -999\n",
    "\n",
    "\n",
    "        img[img == -999] = np.nan\n",
    "\n",
    "        in_shape = img.shape\n",
    "\n",
    "        img = img.reshape(img.shape[0] * img.shape[1])\n",
    "\n",
    "        img = pd.DataFrame(img)\n",
    "\n",
    "        img.columns = min_max.columns\n",
    "\n",
    "        img = pd.concat([min_max, img]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "        img = pd.DataFrame(scaler.fit_transform(img))\n",
    "\n",
    "        img = img.iloc[2:]\n",
    "        img = img.values.reshape(in_shape)\n",
    "\n",
    "        img[np.isnan(img)] = -1\n",
    "\n",
    "\n",
    "        img = np.round(img, 3)\n",
    "        x_val[j] = img#[:, :, 4:] index number is not included, \n",
    "\n",
    "    for j, path in enumerate(val_files):\n",
    "\n",
    "        img =  np.round(np.load(path), 3)[:, :, -1]\n",
    "\n",
    "        img = img.astype(int)\n",
    "\n",
    "        img[img < 0] = 0\n",
    "        img[img >1] = 0\n",
    "        img[~np.isin(img, [0,1])] = 0\n",
    "\n",
    "        img[np.isnan(img)] = 0\n",
    "        img = img.astype(int)\n",
    "\n",
    "        y_val[j] = np.expand_dims(img, 2) \n",
    "        \n",
    "    x_test = np.empty((len(test_files),128,128),  dtype=\"float32\")\n",
    "    y_test = np.empty((len(test_files),128,128,1),  dtype=\"uint8\")\n",
    "    \n",
    "\n",
    "    for j, path in enumerate(test_files):\n",
    "\n",
    "        img =  np.round(np.load(path), 3)[:, :, 6]\n",
    "\n",
    "        img = img.astype(float)\n",
    "        img = np.round(img, 3)\n",
    "        img[img == 0] = -999\n",
    "\n",
    "        img[np.isnan(img)] = -999\n",
    "\n",
    "\n",
    "        img[img == -999] = np.nan\n",
    "\n",
    "        in_shape = img.shape\n",
    "\n",
    "        img = img.reshape(img.shape[0] * img.shape[1])\n",
    "\n",
    "        img = pd.DataFrame(img)\n",
    "\n",
    "        img.columns = min_max.columns\n",
    "\n",
    "        img = pd.concat([min_max, img]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "        img = pd.DataFrame(scaler.fit_transform(img))\n",
    "\n",
    "        img = img.iloc[2:]\n",
    "        img = img.values.reshape(in_shape)\n",
    "\n",
    "        img[np.isnan(img)] = -1\n",
    "\n",
    "\n",
    "        img = np.round(img, 3)\n",
    "\n",
    "        x_test[j] = img#[:, :, 4:] index number is not included, \n",
    "\n",
    "    for j, path in enumerate(test_files):\n",
    "\n",
    "        img =  np.round(np.load(path), 3)[:, :, -1]\n",
    "\n",
    "        img = img.astype(int)\n",
    "\n",
    "        img[img < 0] = 0\n",
    "        img[img >1] = 0\n",
    "        img[~np.isin(img, [0,1])] = 0\n",
    "\n",
    "        img[np.isnan(img)] = 0\n",
    "        img = img.astype(int)\n",
    "\n",
    "        y_test[j] = np.expand_dims(img, 2) \n",
    "        \n",
    "    return x_train,y_train,x_val,y_val,x_test,y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the images based on the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "tensorflow.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 128\n",
    "input_width = 128\n",
    "num_channels = 1\n",
    "n_labels = 1\n",
    "hyperparameters = {\n",
    "    'learning_rate': (1e-5, 1e-2, 'log-uniform'),\n",
    "    'filters': [16, 32, 64],\n",
    "}\n",
    "\n",
    "def build_model(hp):\n",
    "    model = models.unet_plus_2d((None, None, num_channels),\n",
    "                               # filter_num=hp.Int('filter_num', min_value=16, max_value=256, step=16),\n",
    "                               filter_num = [16,32,64, 128, 256],\n",
    "                               activation=hp.Choice('activation', values=['ReLU', 'GELU', 'Snake']),\n",
    "                               n_labels = n_labels,\n",
    "                               stack_num_up = hp.Choice('stack_num_up', values = [1,2]),\n",
    "                               stack_num_down =  hp.Choice('stack_num_down', values = [1,2]),\n",
    "                               output_activation = 'Sigmoid',\n",
    "                               batch_norm = True,\n",
    "                               pool = hp.Choice('pool', values = [True,False]),\n",
    "                               # unpool = hp.Choice('unpool', values = [True,False]),\n",
    "                               unpool = False,\n",
    "                               backbone = hp.Choice('backbone', values = ['EfficientNetB7', 'VGG19', 'ResNet152', 'DenseNet169']),\n",
    "                               # backbone = 'EfficientNetB7'\n",
    "                               weights = None,\n",
    "                               deep_supervision = True\n",
    "                               )\n",
    "        \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# model_unet_from_scratch = models.unet_3plus_2d((None, None, 1), filter_num= [16,32,64,128,256], #make smaller64, 128, 256, 512,[16, 32, 64, 128]\n",
    "#                    n_labels=num_classes, \n",
    "#                    stack_num_down=2, stack_num_up=2, \n",
    "#                    activation='ReLU', \n",
    "#                    output_activation='Sigmoid', \n",
    "#                    batch_norm=True, pool=False, unpool=False, \n",
    "#                    backbone='EfficientNetB7', weights=None, \n",
    "#                    freeze_backbone=False, freeze_batch_norm=False, \n",
    "#                    deep_supervision = True,\n",
    "#                    name='unet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 16:11:51.868293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-18 16:11:55.486031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31011 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n",
      "2023-07-18 16:11:55.520477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31011 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "deep_supervision = True\n",
      "names of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\n",
      "\"final\" is the final output layer):\n",
      "\n",
      "\txnet_output_sup0_activation\n",
      "\txnet_output_sup1_activation\n",
      "\txnet_output_sup2_activation\n",
      "\txnet_output_final_activation\n",
      "Search space summary\n",
      "Default search space size: 6\n",
      "activation (Choice)\n",
      "{'default': 'ReLU', 'conditions': [], 'values': ['ReLU', 'GELU', 'Snake'], 'ordered': False}\n",
      "stack_num_up (Choice)\n",
      "{'default': 1, 'conditions': [], 'values': [1, 2], 'ordered': True}\n",
      "stack_num_down (Choice)\n",
      "{'default': 1, 'conditions': [], 'values': [1, 2], 'ordered': True}\n",
      "pool (Choice)\n",
      "{'default': 1, 'conditions': [], 'values': [1, 0], 'ordered': True}\n",
      "backbone (Choice)\n",
      "{'default': 'EfficientNetB7', 'conditions': [], 'values': ['EfficientNetB7', 'VGG19', 'ResNet152', 'DenseNet169'], 'ordered': False}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.local/lib/python3.10/site-packages/keras_unet_collection/_model_unet_plus_2d.py:246: UserWarning: \n",
      "\n",
      "The shallowest U-net++ deep supervision branch directly connects to a frozen backbone.\n",
      "Testing your configurations on `keras_unet_collection.base.unet_plus_2d_base` is recommended.\n",
      "  warnings.warn(backbone_warn);\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    # objective= kt.Objective('val_mean_iou', direction=\"max\"),  \n",
    "    objective= 'val_loss',  \n",
    "\n",
    "    max_trials=10,\n",
    "    directory='my_directory',\n",
    "    project_name='unet_tuning'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Add the filter_num hyperparameter to the search space\n",
    "# tuner.search_space.update({'filter_num': [16, 32, 64]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " x_train,y_train,x_val,y_val,x_test,y_test = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 29s]\n",
      "val_loss: 11.866268157958984\n",
      "\n",
      "Best val_loss So Far: 2.712986469268799\n",
      "Total elapsed time: 00h 03m 09s\n",
      "\n",
      "Search: Running Trial #6\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "Snake             |ReLU              |activation\n",
      "1                 |2                 |stack_num_up\n",
      "2                 |1                 |stack_num_down\n",
      "1                 |1                 |pool\n",
      "EfficientNetB7    |EfficientNetB7    |backbone\n",
      "0.01              |0.0001            |learning_rate\n",
      "\n",
      "----------\n",
      "deep_supervision = True\n",
      "names of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\n",
      "\"final\" is the final output layer):\n",
      "\n",
      "\txnet_output_sup0_activation\n",
      "\txnet_output_sup1_activation\n",
      "\txnet_output_sup2_activation\n",
      "\txnet_output_final_activation\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "tuner.search(x_train, y_train, epochs=num_epochs, validation_data=(x_val, y_val))\n",
    "\n",
    "# Get the best model hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  activation  stack_num_up  stack_num_down  pool     backbone  learning_rate\n",
      "0       ReLU             1               1     0  DenseNet169           0.01\n"
     ]
    }
   ],
   "source": [
    "final = pd.DataFrame([best_hps.values])\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning3]",
   "language": "python",
   "name": "conda-env-.conda-deeplearning3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
